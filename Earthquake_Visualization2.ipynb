{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable   Type        Data/Info\n",
      "--------------------------------\n",
      "BytesIO    type        <type '_io.BytesIO'>\n",
      "dt         module      <module 'datetime' from '<...>lib-dynload/datetime.so'>\n",
      "np         module      <module 'numpy' from '/ho<...>ages/numpy/__init__.pyc'>\n",
      "urlopen    function    <function urlopen at 0x7f34a4b52b90>\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ad9b4dc5135b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'whos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/sandralaptop/anaconda2/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                       \u001b[1;34m\"pandas from the source directory, you may need to run \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                       \u001b[1;34m\"'python setup.py build_ext --inplace' to build the C \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                       \"extensions first.\".format(module))\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "%whos\n",
    "# import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0fe52781bff6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mholoviews\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sandralaptop/anaconda2/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                       \u001b[1;34m\"pandas from the source directory, you may need to run \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                       \u001b[1;34m\"'python setup.py build_ext --inplace' to build the C \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                       \"extensions first.\".format(module))\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "from urllib2 import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "\n",
    "from matplotlib.image import imread\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "%load_ext holoviews.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%output backend='bokeh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this little demo we'll have a look at the upcoming DataFrame integration and the new bokeh backend for HoloViews and see how we can quickly put together complex visualizations of earthquake data. The DataFrame integration is still work in progress but is usable for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract shape coordinates for the continents and countries from matplotlib's ``basemap`` toolkit and put them inside a ``Polygons`` and ``Contours`` Element respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basemap = Basemap()\n",
    "kdims = ['Longitude', 'Latitude']\n",
    "continents = hv.Polygons([poly.get_coords() for poly in basemap.landpolygons],\n",
    "                         group='Continents', kdims=kdims)\n",
    "countries  = hv.Contours([np.array(country) for path in basemap._readboundarydata('countries')\n",
    "                         for country in path if not isinstance(country, int)],\n",
    "                         group='Countries', kdims=kdims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we can load an satellite image of earth. Unfortunately embedding large images in the notebook using bokeh quickly balloons the size of the notebook so we'll downsample by a factor of 5x here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = basemap.bluemarble()\n",
    "blue_marble = hv.RGB(np.flipud(img.get_array()[::5, ::5]),\n",
    "                     bounds=(-180, -90, 180, 90), kdims=kdims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we download a few months worth of earthquake data from the US Geological survey (USGS), which provides a convenient web API and read it into a pandas DataFrame. For a full reference of the USGS API [look here](http://earthquake.usgs.gov/fdsnws/event/1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a valid query to the USGS API and let pandas handle the loading and parsing of dates \n",
    "query = dict(starttime=\"2014-12-01\", endtime=\"2014-12-31\")\n",
    "query_string = '&'.join('{0}={1}'.format(k, v) for k, v in query.items())\n",
    "query_url = \"http://earthquake.usgs.gov/fdsnws/event/1/query.csv?\" + query_string\n",
    "df = pd.read_csv(BytesIO(urlopen(query_url).read()),\n",
    "                 parse_dates=['time'], index_col='time',\n",
    "                 infer_datetime_format=True)\n",
    "df['Date'] = [str(t)[:19] for t in df.index]\n",
    "\n",
    "# Pass the earthquake dataframe into the HoloViews Element\n",
    "earthquakes = hv.Points(df, kdims=['longitude', 'latitude'],\n",
    "                        vdims=['place', 'Date', 'depth', 'mag', 'rms'],\n",
    "                        group='Earthquakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.reset_index().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what this data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get a summary overview of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 9,000 data points, which should be no problem to load and render in memory. In a future blog post we'll look at loading and dynamically displaying several years worth of data using dask out-of-memory DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Styling our plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some style options, in particular we map the size and color of our points to the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%output size=150\n",
    "%opts Overlay [width=800]\n",
    "%opts Points.Earthquakes [color_index=5 size_index=5 scaling_factor=1.5] (cmap='hot_r' size=1)\n",
    "%opts Polygons.Continents (color='k')\n",
    "%opts Contours.Countries (color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll overlay the earthquake data on top of the 'Blue Marble' image we loaded previous, we'll also enable the hover tool so we can access some more information on each point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%opts Points.Earthquakes [tools=['hover']]\n",
    "blue_marble * earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earthquakes by day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ``groupby`` we can ``split`` our DataFrame up by day and using ``datetime`` we can generate date strings which we'll use as keys in a ``HoloMap``, allowing us to visualize earthquakes for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_df = df.groupby([df.index.year, df.index.month, df.index.day])\n",
    "daily_earthquakes = hv.HoloMap(kdims=['Date'])\n",
    "for date, data in daily_df:\n",
    "    date = str(dt.date(*date))\n",
    "    daily_earthquakes[date] = (continents * countries *\n",
    "                               hv.Points(data, kdims=['longitude', 'latitude'],\n",
    "                                         vdims=['mag'], group='Earthquakes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're trying this notebook out in a live notebook you can set:\n",
    "\n",
    "```python\n",
    "%output widgets='live'\n",
    "```\n",
    "\n",
    "here to update the data dynamically. Since we're embedding this data here we'll only display every third date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%output holomap='scrubber'\n",
    "%%opts Overlay [width=650] Points.Earthquakes [color_index=2 size_index=2] \n",
    "daily_earthquakes[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update: Linked data and widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature I've been playing with is automatic sharing of the data across plots, which automatically allows linked brushing and selecting. Here's a first quick demo of what this can look like. The only thing we need to do when adding a linked Element such as a ``Table`` is to ensure it draws from the same ``DataFrame`` as the other Elements we want to link it with. Using the 'lasso_select' tool we can select only a subregion of points and watch our selection get highlighted in the Table. In reverse we can also highlight rows in the Table and watch our selection appear in the plot, even editing is allowed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%opts Points.Earthquakes [tools=['lasso_select']] Overlay [width=800 height=400] Table [width=800]\n",
    "(blue_marble * earthquakes + hv.Table(earthquakes.data)).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linking plots in this way is a very powerful way to explore high-dimensional data. Here we'll add an Overlay split into tabs plotting the magnitude, RMS and depth value against each other. By linking that with the familiar map, we can easily explore how the geographical location relates to these other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%opts Points [height=250 width=400 tools=['lasso_select', 'box_select']] (unselected_color='indianred')\n",
    "%%opts Overlay [width=500 height=300] Overlay.Combinations [tabs=True]\n",
    "from itertools import combinations\n",
    "dim_combos = combinations(['mag', 'depth', 'rms'], 2)\n",
    "(blue_marble * earthquakes +\n",
    " hv.Overlay([hv.Points(earthquakes.data, kdims=[c1, c2], group='%s_%s' % (c1, c2))\n",
    "            for c1, c2 in dim_combos], group='Combinations')).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this demo. For now you can only try it out yourself if you checkout the 'dataframe' branch on [github.com/ioam/holoviews](https://github.com/ioam/holoviews) and download the notebook below. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
